## ğŸš€ Dropbox-like Cloud Storage System Design (30-Minute Interview Format)

### ğŸ§­ 30-Minute Interview Roadmap

| Time      | Section                       | Goal                                          |
| --------- | ----------------------------- | --------------------------------------------- |
| 0â€“5 min   | ğŸ“œ Requirements Clarification | Nail the problem scope and priorities         |
| 5â€“8 min   | ğŸ”¢ Capacity Estimation        | Show scale awareness with rough numbers       |
| 8â€“16 min  | ğŸ‹ï¸ High-Level Design         | Define system components, data flow, and APIs |
| 16â€“26 min | ğŸ”¬ Deep Dive                  | Explore one or two core components in detail  |
| 26â€“30 min | ğŸ“ˆ Bottlenecks & Scaling      | Address scalability, resilience, and wrap-up  |

---

### =âƒ£ Requirements Clarification (0â€“5 min)

**Ask These to Clarify Scope & Tradeoffs:**

* ğŸ”¢ *Scale*: "How many DAUs and files do we support?"
* â±ï¸ *Latency*: "What's acceptable for sync? <5s?"
* ğŸ“ *Data Types*: "Max file size? Binary vs. text?"
* âš–ï¸ *Tradeoffs*: "Consistency vs. availability?"
* ğŸ”’ *Security*: "How strict are compliance and encryption?"

**Core Functional Requirements (Ranked):**

1. Upload / Download / Delete files
2. Real-time sync across devices
3. Sharing (links, access control)
4. File versioning
5. Search by name/content
6. User auth, quotas, profiles

**Key Non-Functional Constraints:**

* Resumable uploads (large files)
* Strong consistency for file content
* <5s sync latency across devices
* 99.9% availability
* Cost-efficient storage via deduplication
* Encryption at rest + transit

---

### 2âƒ£ Capacity Estimation (5â€“8 min)

**ğŸ§¾ Quick Back-of-the-Envelope Math:**

| Metric             | Value                      |
| ------------------ | -------------------------- |
| **DAUs**           | 10M users                  |
| **Files/User**     | 500 files â†’ 5B total files |
| **Avg File Size**  | 2MB â†’ 10PB storage         |
| **Upload Ops**     | 50M/day â†’ \~1.7K QPS peak  |
| **Download Ops**   | 200M/day â†’ \~7K QPS peak   |
| **Sync Events**    | \~100K/sec globally        |
| **Bandwidth**      | 3.4GB/s up, 14GB/s down    |
| **Storage Growth** | +100TB/year                |
| **Metadata Size**  | \~500GB                    |

**ğŸ“Œ Insight:** Read-heavy (4:1), bursty usage, large scale

---

### 3âƒ£ High-Level Design (8â€“16 min)

#### ğŸ§± Architecture Overview

```
Client Apps â†” CDN â†” Load Balancer â†” API Gateway
                                 â†“
                           Microservices:
                           â”œâ”€â”€ Auth
                           â”œâ”€â”€ Metadata
                           â”œâ”€â”€ Upload
                           â”œâ”€â”€ Sync
                           â”œâ”€â”€ Share
                           â”œâ”€â”€ Search
                           â””â”€â”€ Notify
                                 â†“
                         Kafka/Event Queue
                                 â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚               â”‚               â”‚               â”‚
 PostgreSQL        Redis         S3/Object Store  Elasticsearch
 (Metadata)        (Sessions)    (File Chunks)     (Search)
```

#### ğŸ§¹ Key APIs (Grouped)

**File APIs**

* `POST /files/upload` â€“ Initiate upload
* `PUT /files/{fileId}/chunks/{chunkId}` â€“ Upload chunk
* `POST /files/{fileId}/complete` â€“ Finish upload
* `GET /files/{fileId}` â€“ Download
* `DELETE /files/{fileId}` â€“ Delete

**Sync & Versioning**

* `GET /sync/changes?since={ts}` â€“ Delta sync
* `POST /sync/subscribe` â€“ Real-time updates (WebSocket)
* `GET /files/{fileId}/versions` â€“ Get version history

**Sharing**

* `POST /files/{id}/share` â€“ Create share link
* `GET /shared/{token}` â€“ Access shared file

**Search**

* `GET /search?q={query}` â€“ File/folder search

---

### 4âƒ£ Deep Dive (16â€“26 min)

#### ğŸ”¹ Option A: Chunked File Upload + Deduplication

**Flow:**

1. Client computes SHA256
2. Check if checksum already exists â†’ Deduplicate
3. If not, split into 4MB chunks
4. Upload chunks in parallel
5. Server merges â†’ stores to S3 â†’ updates metadata
6. Notify sync service

**Deduplication Pseudo-code:**

```python
def upload_file(content, user_id):
    checksum = sha256(content)
    if db.exists(checksum):
        link_metadata(user_id, checksum)
        return "Deduped"
    else:
        chunks = split_chunks(content)
        store_chunks(chunks)
        store_metadata(user_id, checksum)
        return "New Upload"
```

#### ğŸ”¹ Option B: Real-time Sync

**Architecture:**

* File change triggers Kafka event
* Sync Service picks it up
* Sends via WebSocket to clients
* Delta updates via `/sync/changes`

**Sync Strategies:**

* **Last-writer wins** (simple)
* **Vector clocks** (accurate, complex)
* **OT/CRDT** (if collaborative editing needed)

#### ğŸ”¹ Option C: Storage Lifecycle Management

* Hot: SSD-backed S3 (recent files)
* Warm: S3 Standard
* Cold: Glacier for archived/unused files
* Use lifecycle rules to auto-move tiers
* Deduplication at block level
* Compress text/doc files

---

### 5âƒ£ Scaling & Wrap-up (26â€“30 min)

#### ğŸ“‰ Bottlenecks & Solutions

| Problem       | Solution                          |
| ------------- | --------------------------------- |
| Metadata DB   | Shard by user, Redis cache, CQRS  |
| Large uploads | Chunked uploads, CDN, retries     |
| Sync storms   | Batch updates, rate limiting      |
| Storage costs | Compression, deduplication, tiers |

#### ğŸ› ï¸ Tech Stack Recap

| Layer       | Technology                |
| ----------- | ------------------------- |
| Frontend    | React, iOS/Android        |
| Backend     | Python/Java microservices |
| API Gateway | Kong / AWS API Gateway    |
| Queue       | Kafka                     |
| DB          | PostgreSQL, Redis         |
| Storage     | S3, Glacier               |
| Search      | Elasticsearch             |
| Monitoring  | Prometheus, Grafana       |

---

## ğŸ’ª Bonus: Follow-Up Q\&A Prep

| â“ Question                  | âœ… Approach                            |
| --------------------------- | ------------------------------------- |
| Offline sync?               | Version vectors, merge logic          |
| Quota enforcement?          | Reject uploads, UI prompt             |
| Collaborative editing?      | OT / CRDT + WebRTC                    |
| Data integrity?             | SHA checksums + periodic audits       |
| File previews?              | Async pipeline + worker queue         |
| Disaster recovery?          | Cross-region replicas, backups        |
| Large file handling (>1GB)? | Streaming upload, CDN, resume support |

---

### âœ… Final Tips

* Start with **upload â†’ sync â†’ share** journey
* Emphasize **data durability**: "no file should ever be lost"
* Show **tradeoff thinking**: consistency vs availability
* Keep **mobile experience** in mind: offline, retries, compression
* Think **globally**: multi-region, CDN-aware, disaster-ready
